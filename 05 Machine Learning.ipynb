{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains the code for the Machine Learning project phase of the Predicting Water Pump Status project.\n",
    "\n",
    "5 algorithms for supervised, multi-classification problems were chosen: KNN, Logistic Regression, RandomForest, AdaBoost and XGBoost.  \n",
    "\n",
    "Data was prepared using one hot encoding, split into training and test sets and scaled.\n",
    "\n",
    "The approach to the model creation and prediction process for using these algorithms followed four steps:\n",
    "1. The 5 algorithms were run on the low-variance dataset.  Feature importances were evaluated for model performance improvement.\n",
    "\n",
    "2. The 5 algorithms were run on the high-variance dataset.  Feature importances were evaluated for model performance improvement. \n",
    "\n",
    "3. Using the dataset that produced the best results, the 5 models were optimized with cross-validation and hyperparameter tuning.\n",
    "\n",
    "4. Using the best dataset and tuned models, principal component analysis was performed for performance improvement.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from pylab import savefig\n",
    "from datetime import datetime\n",
    "from scipy.stats import uniform\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "import xgboost as xgb\n",
    "\n",
    "#functions written for the machine learning phase\n",
    "from ml_support_functions import create_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Calling Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize run parameters\n",
    "results_file = 'C2MLResults_3.csv'\n",
    "df_all_results = pd.DataFrame(columns=['algorithm', 'data note', 'Func F1', 'Non_Func F1', 'Needs_Maint F1', \n",
    "                                       'train_acc', 'test_acc', 'run time','hyper_params'])\n",
    "\n",
    "pd.set_option('display.max_columns', 50)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe shape:  (59399, 12)\n"
     ]
    }
   ],
   "source": [
    "#read the dataset prepared for the machine learning phase\n",
    "dataset_type = 'low_var, ohe'\n",
    "df_orig = pd.read_csv('low_var.csv') #, nrows=5000) \n",
    "\n",
    "df_ml = df_orig.copy()\n",
    "df_ml.drop(df_ml.columns[0], axis=1, inplace=True)\n",
    "\n",
    "print('Dataframe shape: ', df_ml.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform one hot encoding of categorical values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert all features of type object to one hot encoded features. Use the 'drop first' option to avoid creating multicolinearity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(59399, 53)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# change the target feature first so it is not vectorized\n",
    "label_dict_status_group = {'functional':0,\n",
    "                   'non functional': 1,\n",
    "                   'functional needs repair': 2}\n",
    "df_ml['status_group'] = df_ml['status_group'].replace(label_dict_status_group).astype(int)\n",
    "\n",
    "    \n",
    "# Step 1 - generate numeric values for all categorical columns\n",
    "# create a list of the columns of type object\n",
    "ml_datatypes = pd.DataFrame(df_ml.dtypes, columns=['datatype']).reset_index()\n",
    "ml_datatypes.columns=['feature', 'datatype']\n",
    "object_cols = ml_datatypes[ml_datatypes['datatype']=='object']\n",
    "cols = object_cols['feature'].values\n",
    "\n",
    "# factorize each object column\n",
    "for col in cols:\n",
    "    df_ml[col] = pd.factorize(df_ml[col])[0].astype('object')\n",
    "    \n",
    "# Step 2 - create encoded vectors\n",
    "df_ml_enc = pd.get_dummies(df_ml, drop_first=True)\n",
    "df_ml_enc.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One hot encoding changed the number of features from 12 to 53"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the data into train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create numpy arrays for the features and the response variable\n",
    "y = df_ml_enc['status_group'].to_numpy()\n",
    "\n",
    "df_ml_data = df_ml_enc.drop('status_group', axis=1)\n",
    "X = df_ml_data.values\n",
    "colnames = df_ml_data.columns\n",
    "\n",
    "#Split into training and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scale the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_scaler = StandardScaler()\n",
    "X_train = feature_scaler.fit_transform(X_train)\n",
    "X_test = feature_scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Run ML models on Low Variance Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 K Nearest Neighbors - Low Variance\n",
    "\n",
    "This alogrithm predicts the class of a new data point by majority vote of the \"K nearest neighbors\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KNN with default parameters and rule-of-thumb for K\n",
    "\n",
    "K determines how many neighbors are evaluated. With K at the extreme value of 1, we create a rough decision boundary and \"islands\" of data points, overfitting the model. We would get a new decision boundary for each new dataset, making the variance (how much the algoirthm changes given new data) very high.  The bias is very low, because on average the classification performance is accurate.\n",
    "\n",
    "To reduce the roughness of the decision boundary, we increase K and the algorithm uses a majority vote for classification. This reduces the variance, but increases the bias.  If K is too large, we may make the boundary too smooth and make too many mistakes.\n",
    "\n",
    "The rule-of-thumb for K is to use the square root of the number of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion maxtrix: \n",
      " [[5837  624   33]\n",
      " [1955 2567   26]\n",
      " [ 651  123   64]]\n",
      "Classification report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.90      0.78      6494\n",
      "           1       0.77      0.56      0.65      4548\n",
      "           2       0.52      0.08      0.13       838\n",
      "\n",
      "    accuracy                           0.71     11880\n",
      "   macro avg       0.66      0.51      0.52     11880\n",
      "weighted avg       0.71      0.71      0.69     11880\n",
      "\n",
      "Run time:  0:00:51.587570\n"
     ]
    }
   ],
   "source": [
    "k = int(np.sqrt(len(X_train)))\n",
    "knn = KNeighborsClassifier(n_neighbors=k, weights='uniform', algorithm='auto', leaf_size=30, p=2, \n",
    "                           metric='minkowski', metric_params=None, n_jobs=None)\n",
    "\n",
    "knn_model, df_results = create_model('KNeighborsClassifier', knn, dataset_type, X_train, X_test, y_train, y_test, tuning_model='None')\n",
    "df_results.to_csv(results_file, mode='a', header=False)\n",
    "df_all_results = pd.concat([df_all_results, df_results])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model produced a good F1 score for the functional class (0) but scores were not as good for the 'non functional' and 'functional needs maintenance classes'.  This gradient makes sense as it follows the prevalence gradient of those classes in the data. Performing cross-validation could help improve these results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Logistic Regression - Low Variance\n",
    "\n",
    "Logistic regression is the linear model for classification, which models the probabilities of classification outcomes with a logistic function. The error term to minimize is the mean squared error.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion maxtrix: \n",
      " [[5744  735   15]\n",
      " [1882 2656   10]\n",
      " [ 678  135   25]]\n",
      "Classification report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.88      0.78      6494\n",
      "           1       0.75      0.58      0.66      4548\n",
      "           2       0.50      0.03      0.06       838\n",
      "\n",
      "    accuracy                           0.71     11880\n",
      "   macro avg       0.65      0.50      0.50     11880\n",
      "weighted avg       0.70      0.71      0.68     11880\n",
      "\n",
      "Run time:  0:00:01.959735\n"
     ]
    }
   ],
   "source": [
    "log_reg = LogisticRegression(solver='lbfgs', multi_class='auto', max_iter=4000)\n",
    "\n",
    "log_reg_model, df_results = create_model('LogisticRegression', log_reg, dataset_type, X_train, X_test, y_train, y_test)\n",
    "df_results.to_csv(results_file, mode='a', header=False)\n",
    "df_all_results = pd.concat([df_all_results, df_results])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model produced the same F1 score for the 'functional' class, a slightly better score for the 'non functional' class, and a worse score for the 'functional needs repair' class. To improve these results, we will try examining feature importances and re-running the model after dropping features with low coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>construction_year</td>\n",
       "      <td>-0.045578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>gps_height</td>\n",
       "      <td>-0.035450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>extraction_type_class_1</td>\n",
       "      <td>0.004916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>extraction_type_class_4</td>\n",
       "      <td>-0.030253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>management_group_3</td>\n",
       "      <td>0.000861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>payment_type_3</td>\n",
       "      <td>-0.008495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>quantity_2</td>\n",
       "      <td>-0.024171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>quantity_4</td>\n",
       "      <td>-0.041823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>region_1</td>\n",
       "      <td>-0.014981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>region_4</td>\n",
       "      <td>0.013291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>region_15</td>\n",
       "      <td>0.008488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>region_19</td>\n",
       "      <td>-0.044311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>region_20</td>\n",
       "      <td>0.009667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>source_1</td>\n",
       "      <td>-0.020638</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    feature  importance\n",
       "0         construction_year   -0.045578\n",
       "1                gps_height   -0.035450\n",
       "11  extraction_type_class_1    0.004916\n",
       "14  extraction_type_class_4   -0.030253\n",
       "17       management_group_3    0.000861\n",
       "20           payment_type_3   -0.008495\n",
       "22               quantity_2   -0.024171\n",
       "24               quantity_4   -0.041823\n",
       "25                 region_1   -0.014981\n",
       "28                 region_4    0.013291\n",
       "39                region_15    0.008488\n",
       "43                region_19   -0.044311\n",
       "44                region_20    0.009667\n",
       "45                 source_1   -0.020638"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_importances = pd.DataFrame({'feature': colnames, 'importance': log_reg_model.coef_[0]})\n",
    "feature_importances[abs(feature_importances['importance'])<0.05]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion maxtrix: \n",
      " [[5632  862    0]\n",
      " [3390 1158    0]\n",
      " [ 765   73    0]]\n",
      "Classification report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.87      0.69      6494\n",
      "           1       0.55      0.25      0.35      4548\n",
      "           2       0.00      0.00      0.00       838\n",
      "\n",
      "    accuracy                           0.57     11880\n",
      "   macro avg       0.38      0.37      0.35     11880\n",
      "weighted avg       0.53      0.57      0.51     11880\n",
      "\n",
      "Run time:  0:00:00.312436\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alexia/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/alexia/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/alexia/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/alexia/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/alexia/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/alexia/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "# running the model without low-coefficient data \n",
    "bool_mask = abs(feature_importances['importance'])<0.05\n",
    "Xtrain_reduced = X_train[:,bool_mask]\n",
    "Xtest_reduced = X_test[:,bool_mask]\n",
    "\n",
    "log_reg_model, df_results = create_model('LogisticRegression FR', log_reg, dataset_type, Xtrain_reduced, Xtest_reduced, y_train, y_test)\n",
    "df_results.to_csv(results_file, mode='a', header=False)\n",
    "df_all_results = pd.concat([df_all_results, df_results])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results were not improved by reducing features, and the model was not able to produce predictions for the 'functional needs maintenance' class, so the original set of features will be used in future iterations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3. Random Forest - Low Variance\n",
    "This algorithm uses an ensemble of decision trees. The individual trees are formed by branching on each of the features in the dataset. Each leaf is a possible outcome for the feature.  Creating an ensemble avoids the problem of overfitting that commonly occurs in individual trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion maxtrix: \n",
      " [[5594  676  224]\n",
      " [1106 3353   89]\n",
      " [ 497  128  213]]\n",
      "Classification report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.86      0.82      6494\n",
      "           1       0.81      0.74      0.77      4548\n",
      "           2       0.40      0.25      0.31       838\n",
      "\n",
      "    accuracy                           0.77     11880\n",
      "   macro avg       0.66      0.62      0.63     11880\n",
      "weighted avg       0.76      0.77      0.76     11880\n",
      "\n",
      "Run time:  0:00:07.526472\n"
     ]
    }
   ],
   "source": [
    "rfc = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rfc_model, df_results = create_model('Random Forest', rfc, dataset_type, X_train, X_test, y_train, y_test, tuning_model='None')\n",
    "df_results.to_csv(results_file, mode='a', header=False)\n",
    "df_all_results = pd.concat([df_all_results, df_results])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The RandomForest classifier out-performed KNN and LogisticRegression on all classes, and showed a big improvement. In an attempt to improve these results further, we'll try feature reduction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>region_1</td>\n",
       "      <td>0.003108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>region_2</td>\n",
       "      <td>0.002800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>region_3</td>\n",
       "      <td>0.003904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>region_4</td>\n",
       "      <td>0.004091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>region_6</td>\n",
       "      <td>0.002482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>region_7</td>\n",
       "      <td>0.004057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>region_8</td>\n",
       "      <td>0.003619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>region_10</td>\n",
       "      <td>0.003757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>region_11</td>\n",
       "      <td>0.003659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>region_13</td>\n",
       "      <td>0.002709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>region_14</td>\n",
       "      <td>0.003186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>region_15</td>\n",
       "      <td>0.004791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>region_16</td>\n",
       "      <td>0.003640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>region_17</td>\n",
       "      <td>0.003078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>region_18</td>\n",
       "      <td>0.003885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>region_19</td>\n",
       "      <td>0.004729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>region_20</td>\n",
       "      <td>0.000998</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      feature  importance\n",
       "25   region_1    0.003108\n",
       "26   region_2    0.002800\n",
       "27   region_3    0.003904\n",
       "28   region_4    0.004091\n",
       "30   region_6    0.002482\n",
       "31   region_7    0.004057\n",
       "32   region_8    0.003619\n",
       "34  region_10    0.003757\n",
       "35  region_11    0.003659\n",
       "37  region_13    0.002709\n",
       "38  region_14    0.003186\n",
       "39  region_15    0.004791\n",
       "40  region_16    0.003640\n",
       "41  region_17    0.003078\n",
       "42  region_18    0.003885\n",
       "43  region_19    0.004729\n",
       "44  region_20    0.000998"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_importances = pd.DataFrame({'feature': colnames, 'importance': rfc_model.feature_importances_})\n",
    "feature_importances[abs(feature_importances['importance'])<0.005]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion maxtrix: \n",
      " [[5776  718    0]\n",
      " [3551  997    0]\n",
      " [ 759   79    0]]\n",
      "Classification report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.89      0.70      6494\n",
      "           1       0.56      0.22      0.31      4548\n",
      "           2       0.00      0.00      0.00       838\n",
      "\n",
      "    accuracy                           0.57     11880\n",
      "   macro avg       0.38      0.37      0.34     11880\n",
      "weighted avg       0.53      0.57      0.50     11880\n",
      "\n",
      "Run time:  0:00:01.636282\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alexia/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/alexia/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/alexia/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/alexia/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/alexia/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/alexia/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "# running the model without low-coefficient data \n",
    "bool_mask = abs(feature_importances['importance'])<0.005\n",
    "Xtrain_reduced = X_train[:,bool_mask]\n",
    "Xtest_reduced = X_test[:,bool_mask]\n",
    "\n",
    "rfc_model, df_results = create_model('Random Forest FR', rfc, dataset_type, Xtrain_reduced, Xtest_reduced, y_train, y_test)\n",
    "df_results.to_csv(results_file, mode='a', header=False)\n",
    "df_all_results = pd.concat([df_all_results, df_results])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with Logistic Regression, the results were not improved by reducing features, and the model was not able to produce predictions for the 'functional needs maintenance' class, so the original set of features will be used in future iterations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4. AdaBoost - Low Variance\n",
    "The Adaptive Boosting algorithm uses repeated training of weak learners on weighted training data, where incorrect predictions are given more influence, so subsequent learners are directed to focus on examples missed by previous iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion maxtrix: \n",
      " [[5817  663   14]\n",
      " [1860 2666   22]\n",
      " [ 679  137   22]]\n",
      "Classification report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.90      0.78      6494\n",
      "           1       0.77      0.59      0.67      4548\n",
      "           2       0.38      0.03      0.05       838\n",
      "\n",
      "    accuracy                           0.72     11880\n",
      "   macro avg       0.61      0.50      0.50     11880\n",
      "weighted avg       0.70      0.72      0.69     11880\n",
      "\n",
      "Run time:  0:00:03.885111\n"
     ]
    }
   ],
   "source": [
    "abc = AdaBoostClassifier(n_estimators=50, base_estimator=None, learning_rate=1, random_state=42)\n",
    "\n",
    "abc_model, df_results = create_model('AdaBoost default', abc, dataset_type, X_train, X_test, y_train, y_test, tuning_model='None')\n",
    "df_results.to_csv(results_file, mode='a', header=False)\n",
    "df_all_results = pd.concat([df_all_results, df_results])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These results were not as good as the Random Forest classifier. To improve them, we'll try feature reduction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>basin_4</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>basin_6</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>basin_7</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>management_group_1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>quantity_2</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>region_1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>region_2</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>region_3</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>region_4</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>region_6</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>region_11</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>region_15</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>source_1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>source_3</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               feature  importance\n",
       "6              basin_4         0.0\n",
       "8              basin_6         0.0\n",
       "9              basin_7         0.0\n",
       "15  management_group_1         0.0\n",
       "22          quantity_2         0.0\n",
       "25            region_1         0.0\n",
       "26            region_2         0.0\n",
       "27            region_3         0.0\n",
       "28            region_4         0.0\n",
       "30            region_6         0.0\n",
       "35           region_11         0.0\n",
       "39           region_15         0.0\n",
       "45            source_1         0.0\n",
       "47            source_3         0.0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_importances = pd.DataFrame({'feature': colnames, 'importance': abc_model.feature_importances_})\n",
    "feature_importances[abs(feature_importances['importance'])==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion maxtrix: \n",
      " [[5798  696    0]\n",
      " [3721  827    0]\n",
      " [ 774   64    0]]\n",
      "Classification report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.89      0.69      6494\n",
      "           1       0.52      0.18      0.27      4548\n",
      "           2       0.00      0.00      0.00       838\n",
      "\n",
      "    accuracy                           0.56     11880\n",
      "   macro avg       0.36      0.36      0.32     11880\n",
      "weighted avg       0.51      0.56      0.48     11880\n",
      "\n",
      "Run time:  0:00:01.459648\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alexia/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/alexia/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/alexia/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/alexia/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/alexia/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/alexia/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "#running the model without low-coefficient data \n",
    "bool_mask = abs(feature_importances['importance'])==0\n",
    "Xtrain_reduced = X_train[:,bool_mask]\n",
    "Xtest_reduced = X_test[:,bool_mask]\n",
    "\n",
    "abc_model, df_results = create_model('AdaBoost default FR', abc, dataset_type, Xtrain_reduced, Xtest_reduced, y_train, y_test)\n",
    "df_results.to_csv(results_file, mode='a', header=False)\n",
    "df_all_results = pd.concat([df_all_results, df_results])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, the results were not improved by reducing features and the model could not produce predictions for the smallest class. The original set of features will be used going forward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5. XGBoost - Low Variance\n",
    "The Extreme Gradient Boosting algorithm is an implementation of gradient boosted decision trees.  Like Adaptive Boosting, it trains many models sequentially. Each new model gradually minimizes a loss function of the whole system using the Gradient Descent method, an iterative optimization algorithm for finding the minimum of a function. The goal is to create new base learners that are maximally correlated with a negative gradient of the loss function associated with the whole ensemble.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion maxtrix: \n",
      " [[6071  402   21]\n",
      " [1880 2645   23]\n",
      " [ 683   98   57]]\n",
      "Classification report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.93      0.80      6494\n",
      "           1       0.84      0.58      0.69      4548\n",
      "           2       0.56      0.07      0.12       838\n",
      "\n",
      "    accuracy                           0.74     11880\n",
      "   macro avg       0.70      0.53      0.54     11880\n",
      "weighted avg       0.75      0.74      0.71     11880\n",
      "\n",
      "Run time:  0:00:42.314048\n"
     ]
    }
   ],
   "source": [
    "#max_delta_step = 1 helps with tuning for imbalanced dataset where we care about the probability, not AUC \n",
    "xgb_def = xgb.XGBClassifier(max_delta_step = 1, seed=42)\n",
    "\n",
    "model, df_results = create_model('XGBoost default', xgb_def, dataset_type, X_train, X_test, y_train, y_test, tuning_model='None')\n",
    "df_results.to_csv(results_file, mode='a', header=False)\n",
    "df_all_results = pd.concat([df_all_results, df_results])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interestingly, XGBoost did not out-perform the Random Forest classifier. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results so far: all 5 models on the low-variance dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>algorithm</th>\n",
       "      <th>data note</th>\n",
       "      <th>Func F1</th>\n",
       "      <th>Non_Func F1</th>\n",
       "      <th>Needs_Maint F1</th>\n",
       "      <th>train_acc</th>\n",
       "      <th>test_acc</th>\n",
       "      <th>run time</th>\n",
       "      <th>hyper_params</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>KNeighborsClassifier</td>\n",
       "      <td>low_var, ohe</td>\n",
       "      <td>0.781549</td>\n",
       "      <td>0.653015</td>\n",
       "      <td>0.133195</td>\n",
       "      <td>0.710411</td>\n",
       "      <td>0.712795</td>\n",
       "      <td>00:00:51.587570</td>\n",
       "      <td>{'algorithm': 'auto', 'leaf_size': 30, 'metric...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>low_var, ohe</td>\n",
       "      <td>0.776321</td>\n",
       "      <td>0.657914</td>\n",
       "      <td>0.056306</td>\n",
       "      <td>0.709948</td>\n",
       "      <td>0.709175</td>\n",
       "      <td>00:00:01.959735</td>\n",
       "      <td>{'C': 1.0, 'class_weight': None, 'dual': False...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>LogisticRegression FR</td>\n",
       "      <td>low_var, ohe</td>\n",
       "      <td>0.691849</td>\n",
       "      <td>0.348743</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.559944</td>\n",
       "      <td>0.571549</td>\n",
       "      <td>00:00:00.312436</td>\n",
       "      <td>{'C': 1.0, 'class_weight': None, 'dual': False...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>low_var, ohe</td>\n",
       "      <td>0.817179</td>\n",
       "      <td>0.770362</td>\n",
       "      <td>0.312317</td>\n",
       "      <td>0.921884</td>\n",
       "      <td>0.771044</td>\n",
       "      <td>00:00:07.526472</td>\n",
       "      <td>{'bootstrap': True, 'class_weight': None, 'cri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Random Forest FR</td>\n",
       "      <td>low_var, ohe</td>\n",
       "      <td>0.696743</td>\n",
       "      <td>0.314412</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.572171</td>\n",
       "      <td>0.570118</td>\n",
       "      <td>00:00:01.636282</td>\n",
       "      <td>{'bootstrap': True, 'class_weight': None, 'cri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>AdaBoost default</td>\n",
       "      <td>low_var, ohe</td>\n",
       "      <td>0.783434</td>\n",
       "      <td>0.665336</td>\n",
       "      <td>0.049107</td>\n",
       "      <td>0.715272</td>\n",
       "      <td>0.715909</td>\n",
       "      <td>00:00:03.885111</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Non...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>AdaBoost default FR</td>\n",
       "      <td>low_var, ohe</td>\n",
       "      <td>0.690773</td>\n",
       "      <td>0.269601</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.558029</td>\n",
       "      <td>0.557660</td>\n",
       "      <td>00:00:01.459648</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Non...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>XGBoost default</td>\n",
       "      <td>low_var, ohe</td>\n",
       "      <td>0.802618</td>\n",
       "      <td>0.687638</td>\n",
       "      <td>0.121406</td>\n",
       "      <td>0.738126</td>\n",
       "      <td>0.738468</td>\n",
       "      <td>00:00:42.314048</td>\n",
       "      <td>{'base_score': 0.5, 'booster': 'gbtree', 'cols...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               algorithm     data note   Func F1  Non_Func F1  Needs_Maint F1  \\\n",
       "0   KNeighborsClassifier  low_var, ohe  0.781549     0.653015        0.133195   \n",
       "0     LogisticRegression  low_var, ohe  0.776321     0.657914        0.056306   \n",
       "0  LogisticRegression FR  low_var, ohe  0.691849     0.348743        0.000000   \n",
       "0          Random Forest  low_var, ohe  0.817179     0.770362        0.312317   \n",
       "0       Random Forest FR  low_var, ohe  0.696743     0.314412        0.000000   \n",
       "0       AdaBoost default  low_var, ohe  0.783434     0.665336        0.049107   \n",
       "0    AdaBoost default FR  low_var, ohe  0.690773     0.269601        0.000000   \n",
       "0        XGBoost default  low_var, ohe  0.802618     0.687638        0.121406   \n",
       "\n",
       "   train_acc  test_acc        run time  \\\n",
       "0   0.710411  0.712795 00:00:51.587570   \n",
       "0   0.709948  0.709175 00:00:01.959735   \n",
       "0   0.559944  0.571549 00:00:00.312436   \n",
       "0   0.921884  0.771044 00:00:07.526472   \n",
       "0   0.572171  0.570118 00:00:01.636282   \n",
       "0   0.715272  0.715909 00:00:03.885111   \n",
       "0   0.558029  0.557660 00:00:01.459648   \n",
       "0   0.738126  0.738468 00:00:42.314048   \n",
       "\n",
       "                                        hyper_params  \n",
       "0  {'algorithm': 'auto', 'leaf_size': 30, 'metric...  \n",
       "0  {'C': 1.0, 'class_weight': None, 'dual': False...  \n",
       "0  {'C': 1.0, 'class_weight': None, 'dual': False...  \n",
       "0  {'bootstrap': True, 'class_weight': None, 'cri...  \n",
       "0  {'bootstrap': True, 'class_weight': None, 'cri...  \n",
       "0  {'algorithm': 'SAMME.R', 'base_estimator': Non...  \n",
       "0  {'algorithm': 'SAMME.R', 'base_estimator': Non...  \n",
       "0  {'base_score': 0.5, 'booster': 'gbtree', 'cols...  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results so far show that the best-performing algorithm is Random Forest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2. Run the models on the high-variance dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe shape:  (59399, 14)\n"
     ]
    }
   ],
   "source": [
    "#read the dataset prepared for the machine learning phase\n",
    "dataset_type = 'high_var, ohe'\n",
    "df_orig = pd.read_csv('high_var.csv') \n",
    "\n",
    "df_ml = df_orig.copy()\n",
    "df_ml.drop(df_ml.columns[0], axis=1, inplace=True)\n",
    "\n",
    "print('Dataframe shape: ', df_ml.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the target feature first so it is not vectorized\n",
    "label_dict_status_group = {'functional':0,\n",
    "                   'non functional': 1,\n",
    "                   'functional needs repair': 2}\n",
    "df_ml['status_group'] = df_ml['status_group'].replace(label_dict_status_group).astype(int)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess the data by one hot encoding, splitting into test and training sets and scaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1 - generate numeric values for all categorical columns\n",
    "# create a list of the columns of type object\n",
    "ml_datatypes = pd.DataFrame(df_ml.dtypes, columns=['datatype']).reset_index()\n",
    "ml_datatypes.columns=['feature', 'datatype']\n",
    "object_cols = ml_datatypes[ml_datatypes['datatype']=='object']\n",
    "cols = object_cols['feature'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# factorize each object column\n",
    "for col in cols:\n",
    "    df_ml[col] = pd.factorize(df_ml[col])[0].astype('object')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(59399, 56)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 2 - create encoded vectors\n",
    "df_ml_enc = pd.get_dummies(df_ml, drop_first=True)\n",
    "df_ml_enc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Create numpy arrays for the features and the response variable\n",
    "y = df_ml_enc['status_group'].to_numpy()\n",
    "\n",
    "df_ml_data = df_ml_enc.drop('status_group', axis=1)\n",
    "X = df_ml_data.values\n",
    "colnames = df_ml_data.columns\n",
    "\n",
    "#Split into training and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state=42)\n",
    "\n",
    "#Scale\n",
    "feature_scaler = StandardScaler()\n",
    "X_train = feature_scaler.fit_transform(X_train)\n",
    "X_test = feature_scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. KNN - High Variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion maxtrix: \n",
      " [[5885  572   37]\n",
      " [2067 2456   25]\n",
      " [ 637  124   77]]\n",
      "Classification report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.91      0.78      6494\n",
      "           1       0.78      0.54      0.64      4548\n",
      "           2       0.55      0.09      0.16       838\n",
      "\n",
      "    accuracy                           0.71     11880\n",
      "   macro avg       0.67      0.51      0.53     11880\n",
      "weighted avg       0.71      0.71      0.68     11880\n",
      "\n",
      "Run time:  0:01:00.895336\n"
     ]
    }
   ],
   "source": [
    "k = int(np.sqrt(len(X_train)))\n",
    "knn = KNeighborsClassifier(n_neighbors=k, weights='uniform', algorithm='auto', leaf_size=30, p=2, \n",
    "                           metric='minkowski', metric_params=None, n_jobs=None)\n",
    "\n",
    "knn_model, df_results = create_model('KNeighborsClassifier', knn, dataset_type, X_train, X_test, y_train, y_test, tuning_model='None')\n",
    "df_results.to_csv(results_file, mode='a', header=False)\n",
    "df_all_results = pd.concat([df_all_results, df_results])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results of the KNN model created with the high-variance dataset produced mixed results compared to the low-variance dataset.  The 'functional' F1 score stayed the same, the 'non functional' score decreased slightly, and the 'functional needs maintenance' category improved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2.  Logistic Regression - High Variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion maxtrix: \n",
      " [[5817  655   22]\n",
      " [1865 2661   22]\n",
      " [ 694  122   22]]\n",
      "Classification report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.90      0.78      6494\n",
      "           1       0.77      0.59      0.67      4548\n",
      "           2       0.33      0.03      0.05       838\n",
      "\n",
      "    accuracy                           0.72     11880\n",
      "   macro avg       0.60      0.50      0.50     11880\n",
      "weighted avg       0.70      0.72      0.69     11880\n",
      "\n",
      "Run time:  0:00:01.521926\n"
     ]
    }
   ],
   "source": [
    "log_reg = LogisticRegression(solver='lbfgs', multi_class='auto', max_iter=4000)\n",
    "\n",
    "log_reg_model, df_results = create_model('LogisticRegression', log_reg, dataset_type, X_train, X_test, y_train, y_test)\n",
    "df_results.to_csv(results_file, mode='a', header=False)\n",
    "df_all_results = pd.concat([df_all_results, df_results])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results of the Logistic Regression model created with the high-variance dataset produced better results across all categories compared to the low-variance dataset.  It did not however outperform the Random Forest model created with the low-variance dataset.  We'll see if feature reduction improves the outcome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>longitude</td>\n",
       "      <td>0.022257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>region_code</td>\n",
       "      <td>-0.038318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>basin_1</td>\n",
       "      <td>-0.044029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>extraction_type_group_5</td>\n",
       "      <td>-0.011701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>extraction_type_group_6</td>\n",
       "      <td>0.002611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>extraction_type_group_7</td>\n",
       "      <td>-0.006745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>extraction_type_group_9</td>\n",
       "      <td>0.045267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>extraction_type_group_10</td>\n",
       "      <td>0.021762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>extraction_type_group_11</td>\n",
       "      <td>0.031094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>extraction_type_group_12</td>\n",
       "      <td>-0.016162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>management_group_1</td>\n",
       "      <td>0.047664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>management_group_2</td>\n",
       "      <td>0.049897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>management_group_3</td>\n",
       "      <td>0.000350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>management_group_4</td>\n",
       "      <td>0.012976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>payment_type_5</td>\n",
       "      <td>-0.049537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>quantity_4</td>\n",
       "      <td>-0.030473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>source_1</td>\n",
       "      <td>-0.020863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>source_7</td>\n",
       "      <td>-0.035270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>source_8</td>\n",
       "      <td>-0.006960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>source_9</td>\n",
       "      <td>0.002617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>waterpoint_type_2</td>\n",
       "      <td>-0.049912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>waterpoint_type_5</td>\n",
       "      <td>0.035145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>waterpoint_type_6</td>\n",
       "      <td>0.035276</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     feature  importance\n",
       "3                  longitude    0.022257\n",
       "5                region_code   -0.038318\n",
       "6                    basin_1   -0.044029\n",
       "18   extraction_type_group_5   -0.011701\n",
       "19   extraction_type_group_6    0.002611\n",
       "20   extraction_type_group_7   -0.006745\n",
       "22   extraction_type_group_9    0.045267\n",
       "23  extraction_type_group_10    0.021762\n",
       "24  extraction_type_group_11    0.031094\n",
       "25  extraction_type_group_12   -0.016162\n",
       "26        management_group_1    0.047664\n",
       "27        management_group_2    0.049897\n",
       "28        management_group_3    0.000350\n",
       "29        management_group_4    0.012976\n",
       "34            payment_type_5   -0.049537\n",
       "39                quantity_4   -0.030473\n",
       "40                  source_1   -0.020863\n",
       "46                  source_7   -0.035270\n",
       "47                  source_8   -0.006960\n",
       "48                  source_9    0.002617\n",
       "50         waterpoint_type_2   -0.049912\n",
       "53         waterpoint_type_5    0.035145\n",
       "54         waterpoint_type_6    0.035276"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_importances = pd.DataFrame({'feature': colnames, 'importance': log_reg_model.coef_[0]})\n",
    "feature_importances[abs(feature_importances['importance'])<0.05]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion maxtrix: \n",
      " [[5950  536    8]\n",
      " [2859 1678   11]\n",
      " [ 744   91    3]]\n",
      "Classification report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.92      0.74      6494\n",
      "           1       0.73      0.37      0.49      4548\n",
      "           2       0.14      0.00      0.01       838\n",
      "\n",
      "    accuracy                           0.64     11880\n",
      "   macro avg       0.50      0.43      0.41     11880\n",
      "weighted avg       0.63      0.64      0.59     11880\n",
      "\n",
      "Run time:  0:00:00.434861\n"
     ]
    }
   ],
   "source": [
    "# running the model without low-coefficient data \n",
    "bool_mask = abs(feature_importances['importance'])<0.05\n",
    "Xtrain_reduced = X_train[:,bool_mask]\n",
    "Xtest_reduced = X_test[:,bool_mask]\n",
    "\n",
    "log_reg_model, df_results = create_model('LogisticRegression FR', log_reg, dataset_type, Xtrain_reduced, Xtest_reduced, y_train, y_test)\n",
    "df_results.to_csv(results_file, mode='a', header=False)\n",
    "df_all_results = pd.concat([df_all_results, df_results])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the model was able to create predictions for all classes (unlike with the low-variance dataset), reducing the features in the high-variance dataset lead to worse results in the Logistic Regression model for all F1 scores.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3.  Random Forest - High Variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion maxtrix: \n",
      " [[5650  635  209]\n",
      " [ 934 3516   98]\n",
      " [ 433  131  274]]\n",
      "Classification report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.87      0.84      6494\n",
      "           1       0.82      0.77      0.80      4548\n",
      "           2       0.47      0.33      0.39       838\n",
      "\n",
      "    accuracy                           0.79     11880\n",
      "   macro avg       0.70      0.66      0.67     11880\n",
      "weighted avg       0.79      0.79      0.79     11880\n",
      "\n",
      "Run time:  0:00:09.236124\n"
     ]
    }
   ],
   "source": [
    "rfc = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rfc_model, df_results = create_model('Random Forest', rfc, dataset_type, X_train, X_test, y_train, y_test, tuning_model='None')\n",
    "df_results.to_csv(results_file, mode='a', header=False)\n",
    "df_all_results = pd.concat([df_all_results, df_results])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Random Forest model outperformed the Random Forest model created using the low-variance dataset, making this the new top-performing model. Once again we will attempt to improve on these results with feature reduction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>basin_1</td>\n",
       "      <td>0.003567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>basin_2</td>\n",
       "      <td>0.003433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>basin_3</td>\n",
       "      <td>0.003770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>basin_5</td>\n",
       "      <td>0.003663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>basin_6</td>\n",
       "      <td>0.003552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>basin_7</td>\n",
       "      <td>0.004117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>basin_8</td>\n",
       "      <td>0.003812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>extraction_type_group_2</td>\n",
       "      <td>0.004073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>extraction_type_group_4</td>\n",
       "      <td>0.003005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>extraction_type_group_6</td>\n",
       "      <td>0.004831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>extraction_type_group_7</td>\n",
       "      <td>0.000463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>extraction_type_group_8</td>\n",
       "      <td>0.002817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>extraction_type_group_9</td>\n",
       "      <td>0.001115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>extraction_type_group_10</td>\n",
       "      <td>0.000504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>extraction_type_group_11</td>\n",
       "      <td>0.001256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>extraction_type_group_12</td>\n",
       "      <td>0.000386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>management_group_1</td>\n",
       "      <td>0.002235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>management_group_3</td>\n",
       "      <td>0.004305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>management_group_4</td>\n",
       "      <td>0.001808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>payment_type_4</td>\n",
       "      <td>0.003823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>payment_type_5</td>\n",
       "      <td>0.001582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>quantity_4</td>\n",
       "      <td>0.004895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>source_1</td>\n",
       "      <td>0.002235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>source_4</td>\n",
       "      <td>0.000573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>source_7</td>\n",
       "      <td>0.001413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>source_8</td>\n",
       "      <td>0.003004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>source_9</td>\n",
       "      <td>0.000356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>waterpoint_type_4</td>\n",
       "      <td>0.002852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>waterpoint_type_5</td>\n",
       "      <td>0.000594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>waterpoint_type_6</td>\n",
       "      <td>0.000051</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     feature  importance\n",
       "6                    basin_1    0.003567\n",
       "7                    basin_2    0.003433\n",
       "8                    basin_3    0.003770\n",
       "10                   basin_5    0.003663\n",
       "11                   basin_6    0.003552\n",
       "12                   basin_7    0.004117\n",
       "13                   basin_8    0.003812\n",
       "15   extraction_type_group_2    0.004073\n",
       "17   extraction_type_group_4    0.003005\n",
       "19   extraction_type_group_6    0.004831\n",
       "20   extraction_type_group_7    0.000463\n",
       "21   extraction_type_group_8    0.002817\n",
       "22   extraction_type_group_9    0.001115\n",
       "23  extraction_type_group_10    0.000504\n",
       "24  extraction_type_group_11    0.001256\n",
       "25  extraction_type_group_12    0.000386\n",
       "26        management_group_1    0.002235\n",
       "28        management_group_3    0.004305\n",
       "29        management_group_4    0.001808\n",
       "33            payment_type_4    0.003823\n",
       "34            payment_type_5    0.001582\n",
       "39                quantity_4    0.004895\n",
       "40                  source_1    0.002235\n",
       "43                  source_4    0.000573\n",
       "46                  source_7    0.001413\n",
       "47                  source_8    0.003004\n",
       "48                  source_9    0.000356\n",
       "52         waterpoint_type_4    0.002852\n",
       "53         waterpoint_type_5    0.000594\n",
       "54         waterpoint_type_6    0.000051"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_importances = pd.DataFrame({'feature': colnames, 'importance': rfc_model.feature_importances_})\n",
    "feature_importances[abs(feature_importances['importance'])<0.005]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion maxtrix: \n",
      " [[5764  725    5]\n",
      " [3290 1256    2]\n",
      " [ 708  130    0]]\n",
      "Classification report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.89      0.71      6494\n",
      "           1       0.59      0.28      0.38      4548\n",
      "           2       0.00      0.00      0.00       838\n",
      "\n",
      "    accuracy                           0.59     11880\n",
      "   macro avg       0.40      0.39      0.36     11880\n",
      "weighted avg       0.55      0.59      0.53     11880\n",
      "\n",
      "Run time:  0:00:03.373237\n"
     ]
    }
   ],
   "source": [
    "# running the model without low-coefficient data \n",
    "bool_mask = abs(feature_importances['importance'])<0.005\n",
    "Xtrain_reduced = X_train[:,bool_mask]\n",
    "Xtest_reduced = X_test[:,bool_mask]\n",
    "\n",
    "rfc_model, df_results = create_model('Random Forest FR', rfc, dataset_type, Xtrain_reduced, Xtest_reduced, y_train, y_test)\n",
    "df_results.to_csv(results_file, mode='a', header=False)\n",
    "df_all_results = pd.concat([df_all_results, df_results])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While this model was also able to create predictions for all classes (unlike with the low-variance dataset), reducing the features in the high-variance dataset lead to worse results in the Random Forest model for all F1 scores.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4.  AdaBoost - High Variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion maxtrix: \n",
      " [[5876  581   37]\n",
      " [1860 2663   25]\n",
      " [ 654  138   46]]\n",
      "Classification report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.90      0.79      6494\n",
      "           1       0.79      0.59      0.67      4548\n",
      "           2       0.43      0.05      0.10       838\n",
      "\n",
      "    accuracy                           0.72     11880\n",
      "   macro avg       0.64      0.52      0.52     11880\n",
      "weighted avg       0.71      0.72      0.70     11880\n",
      "\n",
      "Run time:  0:00:05.297490\n"
     ]
    }
   ],
   "source": [
    "abc = AdaBoostClassifier(n_estimators=50, base_estimator=None, learning_rate=1, random_state=42)\n",
    "\n",
    "abc_model, df_results = create_model('AdaBoost default', abc, dataset_type, X_train, X_test, y_train, y_test, tuning_model='None')\n",
    "df_results.to_csv(results_file, mode='a', header=False)\n",
    "df_all_results = pd.concat([df_all_results, df_results])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While this AdaBoost model produced better results than with the low-variance dataset, it did not out-perform the best model so far. We'll try feature reduction, just in case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>permit</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>basin_1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>basin_3</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>basin_5</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>basin_6</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>basin_7</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>extraction_type_group_1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>extraction_type_group_2</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>extraction_type_group_4</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>extraction_type_group_6</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>extraction_type_group_7</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>extraction_type_group_8</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>extraction_type_group_9</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>extraction_type_group_10</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>extraction_type_group_11</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>extraction_type_group_12</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>management_group_1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>management_group_3</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>management_group_4</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>payment_type_4</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>payment_type_5</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>quantity_2</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>source_4</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>source_9</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>waterpoint_type_4</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>waterpoint_type_5</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>waterpoint_type_6</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     feature  importance\n",
       "4                     permit         0.0\n",
       "6                    basin_1         0.0\n",
       "8                    basin_3         0.0\n",
       "10                   basin_5         0.0\n",
       "11                   basin_6         0.0\n",
       "12                   basin_7         0.0\n",
       "14   extraction_type_group_1         0.0\n",
       "15   extraction_type_group_2         0.0\n",
       "17   extraction_type_group_4         0.0\n",
       "19   extraction_type_group_6         0.0\n",
       "20   extraction_type_group_7         0.0\n",
       "21   extraction_type_group_8         0.0\n",
       "22   extraction_type_group_9         0.0\n",
       "23  extraction_type_group_10         0.0\n",
       "24  extraction_type_group_11         0.0\n",
       "25  extraction_type_group_12         0.0\n",
       "26        management_group_1         0.0\n",
       "28        management_group_3         0.0\n",
       "29        management_group_4         0.0\n",
       "33            payment_type_4         0.0\n",
       "34            payment_type_5         0.0\n",
       "37                quantity_2         0.0\n",
       "43                  source_4         0.0\n",
       "48                  source_9         0.0\n",
       "52         waterpoint_type_4         0.0\n",
       "53         waterpoint_type_5         0.0\n",
       "54         waterpoint_type_6         0.0"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_importances = pd.DataFrame({'feature': colnames, 'importance': abc_model.feature_importances_})\n",
    "feature_importances[abs(feature_importances['importance'])==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion maxtrix: \n",
      " [[6137  357    0]\n",
      " [4045  503    0]\n",
      " [ 787   51    0]]\n",
      "Classification report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.95      0.70      6494\n",
      "           1       0.55      0.11      0.18      4548\n",
      "           2       0.00      0.00      0.00       838\n",
      "\n",
      "    accuracy                           0.56     11880\n",
      "   macro avg       0.37      0.35      0.30     11880\n",
      "weighted avg       0.52      0.56      0.45     11880\n",
      "\n",
      "Run time:  0:00:01.858753\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alexia/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/alexia/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/alexia/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/alexia/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/alexia/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/alexia/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "#running the model without low-coefficient data \n",
    "bool_mask = abs(feature_importances['importance'])==0\n",
    "Xtrain_reduced = X_train[:,bool_mask]\n",
    "Xtest_reduced = X_test[:,bool_mask]\n",
    "\n",
    "abc_model, df_results = create_model('AdaBoost default FR', abc, dataset_type, Xtrain_reduced, Xtest_reduced, y_train, y_test)\n",
    "df_results.to_csv(results_file, mode='a', header=False)\n",
    "df_all_results = pd.concat([df_all_results, df_results])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, feature reduction did not produce better results and rather lead to lower results and the inability to predict for the smallest class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5. XGBoost - High Variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion maxtrix: \n",
      " [[6067  406   21]\n",
      " [1839 2688   21]\n",
      " [ 664  107   67]]\n",
      "Classification report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.93      0.81      6494\n",
      "           1       0.84      0.59      0.69      4548\n",
      "           2       0.61      0.08      0.14       838\n",
      "\n",
      "    accuracy                           0.74     11880\n",
      "   macro avg       0.72      0.54      0.55     11880\n",
      "weighted avg       0.75      0.74      0.72     11880\n",
      "\n",
      "Run time:  0:00:46.615755\n"
     ]
    }
   ],
   "source": [
    "#max_delta_step = 1 helps with tuning for imbalanced dataset where we care about the probability, not AUC \n",
    "xgb_def = xgb.XGBClassifier(max_delta_step = 1, seed=42)\n",
    "\n",
    "model, df_results = create_model('XGBoost default', xgb_def, dataset_type, X_train, X_test, y_train, y_test, tuning_model='None')\n",
    "df_results.to_csv(results_file, mode='a', header=False)\n",
    "df_all_results = pd.concat([df_all_results, df_results])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The XGBoost model improved slightly across all categories from the low-variance dataset, but did not out-perform the Random Forest model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results of the high and low variance datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>algorithm</th>\n",
       "      <th>data note</th>\n",
       "      <th>Func F1</th>\n",
       "      <th>Non_Func F1</th>\n",
       "      <th>Needs_Maint F1</th>\n",
       "      <th>train_acc</th>\n",
       "      <th>test_acc</th>\n",
       "      <th>run time</th>\n",
       "      <th>hyper_params</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>KNeighborsClassifier</td>\n",
       "      <td>low_var, ohe</td>\n",
       "      <td>0.781549</td>\n",
       "      <td>0.653015</td>\n",
       "      <td>0.133195</td>\n",
       "      <td>0.710411</td>\n",
       "      <td>0.712795</td>\n",
       "      <td>00:00:51.587570</td>\n",
       "      <td>{'algorithm': 'auto', 'leaf_size': 30, 'metric...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>low_var, ohe</td>\n",
       "      <td>0.776321</td>\n",
       "      <td>0.657914</td>\n",
       "      <td>0.056306</td>\n",
       "      <td>0.709948</td>\n",
       "      <td>0.709175</td>\n",
       "      <td>00:00:01.959735</td>\n",
       "      <td>{'C': 1.0, 'class_weight': None, 'dual': False...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>LogisticRegression FR</td>\n",
       "      <td>low_var, ohe</td>\n",
       "      <td>0.691849</td>\n",
       "      <td>0.348743</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.559944</td>\n",
       "      <td>0.571549</td>\n",
       "      <td>00:00:00.312436</td>\n",
       "      <td>{'C': 1.0, 'class_weight': None, 'dual': False...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>low_var, ohe</td>\n",
       "      <td>0.817179</td>\n",
       "      <td>0.770362</td>\n",
       "      <td>0.312317</td>\n",
       "      <td>0.921884</td>\n",
       "      <td>0.771044</td>\n",
       "      <td>00:00:07.526472</td>\n",
       "      <td>{'bootstrap': True, 'class_weight': None, 'cri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Random Forest FR</td>\n",
       "      <td>low_var, ohe</td>\n",
       "      <td>0.696743</td>\n",
       "      <td>0.314412</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.572171</td>\n",
       "      <td>0.570118</td>\n",
       "      <td>00:00:01.636282</td>\n",
       "      <td>{'bootstrap': True, 'class_weight': None, 'cri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>AdaBoost default</td>\n",
       "      <td>low_var, ohe</td>\n",
       "      <td>0.783434</td>\n",
       "      <td>0.665336</td>\n",
       "      <td>0.049107</td>\n",
       "      <td>0.715272</td>\n",
       "      <td>0.715909</td>\n",
       "      <td>00:00:03.885111</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Non...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>AdaBoost default FR</td>\n",
       "      <td>low_var, ohe</td>\n",
       "      <td>0.690773</td>\n",
       "      <td>0.269601</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.558029</td>\n",
       "      <td>0.557660</td>\n",
       "      <td>00:00:01.459648</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Non...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>XGBoost default</td>\n",
       "      <td>low_var, ohe</td>\n",
       "      <td>0.802618</td>\n",
       "      <td>0.687638</td>\n",
       "      <td>0.121406</td>\n",
       "      <td>0.738126</td>\n",
       "      <td>0.738468</td>\n",
       "      <td>00:00:42.314048</td>\n",
       "      <td>{'base_score': 0.5, 'booster': 'gbtree', 'cols...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>KNeighborsClassifier</td>\n",
       "      <td>high_var, ohe</td>\n",
       "      <td>0.780349</td>\n",
       "      <td>0.637922</td>\n",
       "      <td>0.157625</td>\n",
       "      <td>0.710579</td>\n",
       "      <td>0.708586</td>\n",
       "      <td>00:01:00.895336</td>\n",
       "      <td>{'algorithm': 'auto', 'leaf_size': 30, 'metric...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>high_var, ohe</td>\n",
       "      <td>0.782381</td>\n",
       "      <td>0.666416</td>\n",
       "      <td>0.048673</td>\n",
       "      <td>0.718828</td>\n",
       "      <td>0.715488</td>\n",
       "      <td>00:00:01.521926</td>\n",
       "      <td>{'C': 1.0, 'class_weight': None, 'dual': False...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>LogisticRegression FR</td>\n",
       "      <td>high_var, ohe</td>\n",
       "      <td>0.741572</td>\n",
       "      <td>0.489713</td>\n",
       "      <td>0.006977</td>\n",
       "      <td>0.636966</td>\n",
       "      <td>0.642340</td>\n",
       "      <td>00:00:00.434861</td>\n",
       "      <td>{'C': 1.0, 'class_weight': None, 'dual': False...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>high_var, ohe</td>\n",
       "      <td>0.836356</td>\n",
       "      <td>0.796376</td>\n",
       "      <td>0.386187</td>\n",
       "      <td>0.994234</td>\n",
       "      <td>0.794613</td>\n",
       "      <td>00:00:09.236124</td>\n",
       "      <td>{'bootstrap': True, 'class_weight': None, 'cri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Random Forest FR</td>\n",
       "      <td>high_var, ohe</td>\n",
       "      <td>0.709154</td>\n",
       "      <td>0.377234</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.594268</td>\n",
       "      <td>0.590909</td>\n",
       "      <td>00:00:03.373237</td>\n",
       "      <td>{'bootstrap': True, 'class_weight': None, 'cri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>AdaBoost default</td>\n",
       "      <td>high_var, ohe</td>\n",
       "      <td>0.789573</td>\n",
       "      <td>0.671627</td>\n",
       "      <td>0.097252</td>\n",
       "      <td>0.723563</td>\n",
       "      <td>0.722643</td>\n",
       "      <td>00:00:05.297490</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Non...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>AdaBoost default FR</td>\n",
       "      <td>high_var, ohe</td>\n",
       "      <td>0.702857</td>\n",
       "      <td>0.184283</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.556009</td>\n",
       "      <td>0.558923</td>\n",
       "      <td>00:00:01.858753</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Non...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>XGBoost default</td>\n",
       "      <td>high_var, ohe</td>\n",
       "      <td>0.805497</td>\n",
       "      <td>0.693767</td>\n",
       "      <td>0.141499</td>\n",
       "      <td>0.742734</td>\n",
       "      <td>0.742593</td>\n",
       "      <td>00:00:46.615755</td>\n",
       "      <td>{'base_score': 0.5, 'booster': 'gbtree', 'cols...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               algorithm      data note   Func F1  Non_Func F1  \\\n",
       "0   KNeighborsClassifier   low_var, ohe  0.781549     0.653015   \n",
       "0     LogisticRegression   low_var, ohe  0.776321     0.657914   \n",
       "0  LogisticRegression FR   low_var, ohe  0.691849     0.348743   \n",
       "0          Random Forest   low_var, ohe  0.817179     0.770362   \n",
       "0       Random Forest FR   low_var, ohe  0.696743     0.314412   \n",
       "0       AdaBoost default   low_var, ohe  0.783434     0.665336   \n",
       "0    AdaBoost default FR   low_var, ohe  0.690773     0.269601   \n",
       "0        XGBoost default   low_var, ohe  0.802618     0.687638   \n",
       "0   KNeighborsClassifier  high_var, ohe  0.780349     0.637922   \n",
       "0     LogisticRegression  high_var, ohe  0.782381     0.666416   \n",
       "0  LogisticRegression FR  high_var, ohe  0.741572     0.489713   \n",
       "0          Random Forest  high_var, ohe  0.836356     0.796376   \n",
       "0       Random Forest FR  high_var, ohe  0.709154     0.377234   \n",
       "0       AdaBoost default  high_var, ohe  0.789573     0.671627   \n",
       "0    AdaBoost default FR  high_var, ohe  0.702857     0.184283   \n",
       "0        XGBoost default  high_var, ohe  0.805497     0.693767   \n",
       "\n",
       "   Needs_Maint F1  train_acc  test_acc        run time  \\\n",
       "0        0.133195   0.710411  0.712795 00:00:51.587570   \n",
       "0        0.056306   0.709948  0.709175 00:00:01.959735   \n",
       "0        0.000000   0.559944  0.571549 00:00:00.312436   \n",
       "0        0.312317   0.921884  0.771044 00:00:07.526472   \n",
       "0        0.000000   0.572171  0.570118 00:00:01.636282   \n",
       "0        0.049107   0.715272  0.715909 00:00:03.885111   \n",
       "0        0.000000   0.558029  0.557660 00:00:01.459648   \n",
       "0        0.121406   0.738126  0.738468 00:00:42.314048   \n",
       "0        0.157625   0.710579  0.708586 00:01:00.895336   \n",
       "0        0.048673   0.718828  0.715488 00:00:01.521926   \n",
       "0        0.006977   0.636966  0.642340 00:00:00.434861   \n",
       "0        0.386187   0.994234  0.794613 00:00:09.236124   \n",
       "0        0.000000   0.594268  0.590909 00:00:03.373237   \n",
       "0        0.097252   0.723563  0.722643 00:00:05.297490   \n",
       "0        0.000000   0.556009  0.558923 00:00:01.858753   \n",
       "0        0.141499   0.742734  0.742593 00:00:46.615755   \n",
       "\n",
       "                                        hyper_params  \n",
       "0  {'algorithm': 'auto', 'leaf_size': 30, 'metric...  \n",
       "0  {'C': 1.0, 'class_weight': None, 'dual': False...  \n",
       "0  {'C': 1.0, 'class_weight': None, 'dual': False...  \n",
       "0  {'bootstrap': True, 'class_weight': None, 'cri...  \n",
       "0  {'bootstrap': True, 'class_weight': None, 'cri...  \n",
       "0  {'algorithm': 'SAMME.R', 'base_estimator': Non...  \n",
       "0  {'algorithm': 'SAMME.R', 'base_estimator': Non...  \n",
       "0  {'base_score': 0.5, 'booster': 'gbtree', 'cols...  \n",
       "0  {'algorithm': 'auto', 'leaf_size': 30, 'metric...  \n",
       "0  {'C': 1.0, 'class_weight': None, 'dual': False...  \n",
       "0  {'C': 1.0, 'class_weight': None, 'dual': False...  \n",
       "0  {'bootstrap': True, 'class_weight': None, 'cri...  \n",
       "0  {'bootstrap': True, 'class_weight': None, 'cri...  \n",
       "0  {'algorithm': 'SAMME.R', 'base_estimator': Non...  \n",
       "0  {'algorithm': 'SAMME.R', 'base_estimator': Non...  \n",
       "0  {'base_score': 0.5, 'booster': 'gbtree', 'cols...  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on these results, the models performed better on the high-variance dataset.  This will be used in the next attempt to improve the outcomes, with hyperparamter tuning.  The Random Forest model had the highest scores, marking the scores to beat."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Hyperparameter Tuning on the high-variance dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The KNN model was out-performed by the other 4 models, and will not be included in this step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Logistic Regression with hyperparameter tuning and cross validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regularization adds a term to the mean squared error formula, penalizing large coefficients, helping to prevent overfitting. It is applied by default, but can be customized with parameters.\n",
    "\n",
    "C controls the inverse of the regularization strength. A large C can lead to an overfit model, while a small C can lead to an underfit model.\n",
    "\n",
    "L1 minimizes the sum of the absolute differences (S) between the target value (Yi) and the estimated values (f(xi)), aka \"least absolute deviations\".  L2 minimizing the sum of the square of the differences (S) between the target value (Yi) and the estimated values (f(xi), aka \"least squares error\".\n",
    "\n",
    "The liblinear solver is good for small datasets.\n",
    "\n",
    "The balanced mode of the class_weight parameter uses the values of Y to automatically adjust weights inversely proportional to class frequencies in the input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegression(max_iter=4000, multi_class='auto')\n",
    "\n",
    "# Create regularization penalty space\n",
    "penalty_list = ['l1', 'l2']\n",
    "c_space = np.logspace(-5, 10, 15)\n",
    "\n",
    "# Create solver list \n",
    "solver_list = ['liblinear',  'saga']\n",
    "    \n",
    "# try balancing / not balancing\n",
    "class_weight_list = ['balanced', None]\n",
    "\n",
    "hyper_logreg = dict(penalty=penalty_list, C=c_space, solver = solver_list,\n",
    "        class_weight=class_weight_list)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### i. Logistic Regression with grid search hyperparameter tuning and cross validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model with default parameters took 1 minute, and we are running with 120 combinations of parameters, so this should take at least 2 hours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters:  [('C', 0.016378937069540647), ('class_weight', None), ('penalty', 'l1'), ('solver', 'liblinear')]\n",
      "Confusion maxtrix: \n",
      " [[5896  595    3]\n",
      " [1913 2628    7]\n",
      " [ 711  118    9]]\n",
      "Classification report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.91      0.79      6494\n",
      "           1       0.79      0.58      0.67      4548\n",
      "           2       0.47      0.01      0.02       838\n",
      "\n",
      "    accuracy                           0.72     11880\n",
      "   macro avg       0.65      0.50      0.49     11880\n",
      "weighted avg       0.71      0.72      0.69     11880\n",
      "\n",
      "Run time:  2:39:31.785908\n"
     ]
    }
   ],
   "source": [
    "model, df_results = create_model('LogReg GridSearch', logreg, dataset_type, X_train, X_test, y_train, y_test,\n",
    "                                               tuning_model = 'Grid', hyperparams = hyper_logreg)\n",
    "df_results.to_csv(results_file, mode='a', header=False)\n",
    "df_all_results = pd.concat([df_all_results, df_results])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compared to the defaul Logistic Regression model, this model produced the same F1 scores for the first two classes, and did slightly worse on the smallest class.  The runtime was 30 minutes longer than anticipated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ii. Logistic Regression with random search hyperparameter tuning and cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters:  [('C', 2.2758459260747865), ('class_weight', None), ('penalty', 'l1'), ('solver', 'liblinear')]\n",
      "Confusion maxtrix: \n",
      " [[5835  652    7]\n",
      " [1873 2667    8]\n",
      " [ 707  120   11]]\n",
      "Classification report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.90      0.78      6494\n",
      "           1       0.78      0.59      0.67      4548\n",
      "           2       0.42      0.01      0.03       838\n",
      "\n",
      "    accuracy                           0.72     11880\n",
      "   macro avg       0.63      0.50      0.49     11880\n",
      "weighted avg       0.71      0.72      0.69     11880\n",
      "\n",
      "Run time:  0:18:14.166663\n"
     ]
    }
   ],
   "source": [
    "model, df_results = create_model('LogReg RandomSearch', logreg, dataset_type, X_train, X_test, y_train, y_test,\n",
    "                                               tuning_model = 'Random', hyperparams = hyper_logreg)\n",
    "df_results.to_csv(results_file, mode='a', header=False)\n",
    "df_all_results = pd.concat([df_all_results, df_results])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compared to the defaul model, the tuning lead to slightly worse class 1 results, the same class 2 results and a slightly better class 3 result.  The runtime was significantly faster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Random Forest with hyperparameter tuning and cross validation\n",
    "\n",
    "n_estimators is the number of trees in the forest. The larger the better, but this increases the computational cost, and improvements will plateau.\n",
    "\n",
    "max_features is the size of the random subsets of features to consider when splitting a node. Low numbers reduce variance but increase bias. The rule of thumb for classification problems is to use the square root of the number of features. \n",
    "\n",
    "Bootstrap samples are used by default, setting this parameter to false will use the entire dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc = RandomForestClassifier(random_state=42)\n",
    "\n",
    "n_estimator_list = [800, 1000, 1500]\n",
    "max_features_size = ['sqrt']\n",
    "bootstrap_list = [True, False]\n",
    "\n",
    "hyper_rfc = dict(n_estimators=n_estimator_list, max_features=max_features_size, bootstrap=bootstrap_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### i. Random Forest with grid search hyperparameter tuning and cross validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The default Random Forest model took 9 minutes. We are running 6 parameter scenarios, but the time increase might not be linear, since increasing numbers of estimators will take longer (the default is 100). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters:  [('bootstrap', True), ('max_features', 'sqrt'), ('n_estimators', 800)]\n",
      "Confusion maxtrix: \n",
      " [[5654  637  203]\n",
      " [ 932 3521   95]\n",
      " [ 434  130  274]]\n",
      "Classification report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.87      0.84      6494\n",
      "           1       0.82      0.77      0.80      4548\n",
      "           2       0.48      0.33      0.39       838\n",
      "\n",
      "    accuracy                           0.80     11880\n",
      "   macro avg       0.70      0.66      0.67     11880\n",
      "weighted avg       0.79      0.80      0.79     11880\n",
      "\n",
      "Run time:  0:56:09.165509\n"
     ]
    }
   ],
   "source": [
    "model, df_results = create_model('Random Forest GridSearch', rfc, dataset_type, X_train, X_test, y_train, y_test,\n",
    "                                               tuning_model = 'Grid', hyperparams = hyper_rfc)\n",
    "df_results.to_csv(results_file, mode='a', header=False)\n",
    "df_all_results = pd.concat([df_all_results, df_results])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tuned model produced roughly the same results, with improvements seen at 4 demical places. The runtime was roughly a linear change from the default model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### i. Random Forest with random search hyperparameter tuning and cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alexia/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_search.py:266: UserWarning: The total space of parameters 6 is smaller than n_iter=15. Running 6 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  % (grid_size, self.n_iter, grid_size), UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters:  [('bootstrap', True), ('max_features', 'sqrt'), ('n_estimators', 800)]\n",
      "Confusion maxtrix: \n",
      " [[5654  637  203]\n",
      " [ 932 3521   95]\n",
      " [ 434  130  274]]\n",
      "Classification report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.87      0.84      6494\n",
      "           1       0.82      0.77      0.80      4548\n",
      "           2       0.48      0.33      0.39       838\n",
      "\n",
      "    accuracy                           0.80     11880\n",
      "   macro avg       0.70      0.66      0.67     11880\n",
      "weighted avg       0.79      0.80      0.79     11880\n",
      "\n",
      "Run time:  3:39:01.587224\n"
     ]
    }
   ],
   "source": [
    "model, df_results = create_model('Random Forest Random Search', rfc, dataset_type, X_train, X_test, y_train, y_test,\n",
    "                                               tuning_model = 'Random', hyperparams = hyper_rfc)\n",
    "df_results.to_csv(results_file, mode='a', header=False)\n",
    "df_all_results = pd.concat([df_all_results, df_results])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model also produced roughly the same results, but took over three times as long. That could be due to an anomoly with the local CPU, or a complication created by providing a space of parameters smaller than the number of iterations specified."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3. AdaBoost with hyperparameter tuning and cross validation\n",
    "\n",
    "n_estimators is the maximum number of weak learners to create.  The default type of weak learner is DecisionTreeClassifier.\n",
    "\n",
    "Learning rate shrinks the contribution of each classifier.\n",
    "\n",
    "The algorithm list allows for calculating class probabilities or discrete values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "abc = AdaBoostClassifier(random_state=42)\n",
    "\n",
    "n_estimator_list = [150, 200, 250] \n",
    "learning_rate_list = [1, 0.5, 0.25]\n",
    "algorithm_list = ['SAMME.R', 'SAMME']  \n",
    "hyper_abc = dict(n_estimators=n_estimator_list, learning_rate=learning_rate_list, algorithm = algorithm_list)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### i. AdaBoost with grid search hyperparameter tuning and cross validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The adaboost model with default parameters took 5 minutes, and we are running with 9 combinations of parameters, so this should take about 1hr."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters:  [('algorithm', 'SAMME.R'), ('learning_rate', 1), ('n_estimators', 250)]\n",
      "Confusion maxtrix: \n",
      " [[5802  643   49]\n",
      " [1710 2810   28]\n",
      " [ 640  144   54]]\n",
      "Classification report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.89      0.79      6494\n",
      "           1       0.78      0.62      0.69      4548\n",
      "           2       0.41      0.06      0.11       838\n",
      "\n",
      "    accuracy                           0.73     11880\n",
      "   macro avg       0.64      0.53      0.53     11880\n",
      "weighted avg       0.72      0.73      0.71     11880\n",
      "\n",
      "Run time:  6:14:02.121657\n"
     ]
    }
   ],
   "source": [
    "model, df_results = create_model('AdaBoost GridSearch', abc, dataset_type, X_train, X_test, y_train, y_test,\n",
    "                                               tuning_model = 'Grid', hyperparams = hyper_abc)\n",
    "df_results.to_csv(results_file, mode='a', header=False)\n",
    "df_all_results = pd.concat([df_all_results, df_results])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tuned model produced better results for all classifications compared to the default model.  The run time was 6 times the estimate.  Since the best n_estimators parameter was the value at the top of the range, we should re-run the tuning with a higher range.  That will be left for future work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ii. AdaBoost with random search hyperparameter tuning and cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters:  [('algorithm', 'SAMME.R'), ('learning_rate', 1), ('n_estimators', 250)]\n",
      "Confusion maxtrix: \n",
      " [[5802  643   49]\n",
      " [1710 2810   28]\n",
      " [ 640  144   54]]\n",
      "Classification report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.89      0.79      6494\n",
      "           1       0.78      0.62      0.69      4548\n",
      "           2       0.41      0.06      0.11       838\n",
      "\n",
      "    accuracy                           0.73     11880\n",
      "   macro avg       0.64      0.53      0.53     11880\n",
      "weighted avg       0.72      0.73      0.71     11880\n",
      "\n",
      "Run time:  1:28:15.637785\n"
     ]
    }
   ],
   "source": [
    "model, df_results = create_model('AdaBoost RandomSearch', abc, dataset_type, X_train, X_test, y_train, y_test,\n",
    "                                               tuning_model = 'Random', hyperparams = hyper_abc)\n",
    "df_results.to_csv(results_file, mode='a', header=False)\n",
    "df_all_results = pd.concat([df_all_results, df_results])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model produced the same results as the grid search for AdaBoost, but ran in about 1/6 of the time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 XGBoost with hyperparameter tuning and cross validation\n",
    "XGBoost exposes many parameters that can be tuned. An subset most appliccable to this dataset has been chosen as follows:\n",
    "\n",
    "n_estimators is the maximum number of weak learners to create.  The default type of weak learner is gbtree, a tree based model.\n",
    "\n",
    "Learning rate shrinks the contribution of each classifier.\n",
    "\n",
    "Controlling overfitting:\n",
    "\n",
    "1) Control model complexity with \n",
    "* max_depth - maximum depth of the tree. Inceasing this value increases complexity and risks overfitting.\n",
    "* min_child_weight - if a partition step results in a leaf node with the sum of instance weight less than this parameter, partitioning stops. Higher values reduce complexity.\n",
    "* gamma - minimum loss reduction required to make a further partition. Higher gamma reduces complexity.\n",
    "* reg_alpha - L1 regularization term on weights. Higher values reduce complexity.\n",
    "\n",
    "2) Add randomness to make training resistant to effect of noise in the data with \n",
    "* subsample - the proportion of training data to randomly sample prior to growing trees to prevent overfitting.\n",
    "* colsample_bytree - the subsample ratio of columns to use when constructing each tree, to prevent overfitting.\n",
    "\n",
    "Because of the large number of hyperparameters available, parameter tuning will be done in batches, using tuned parameters in subsequent iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_estimator_list = [400, 600, 800]\n",
    "learning_rate_list = [0.01, 0.1, 1]\n",
    "max_depth_list = [3, 5, 7]\n",
    "min_child_weight_list = [0, 2, 4]\n",
    "gamma_list = [0, 1]\n",
    "subsample_list = [i/10.0 for i in range(6,10)]\n",
    "colsample_bytree_list = [i/10.0 for i in range(3,5)]\n",
    "reg_alpha_list = [0, 0.001, 0.005, 0.01]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The runtime for the default XGBoost was about 45 minutes. We are running over 9 parameter scenarios, and given the proximity of results between gris search and random search outputs so far, will run a random search to save time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters:  [('learning_rate', 0.1), ('max_depth', 7), ('n_estimators', 800)]\n",
      "Confusion maxtrix: \n",
      " [[5823  555  116]\n",
      " [1048 3443   57]\n",
      " [ 482  132  224]]\n",
      "Classification report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.90      0.84      6494\n",
      "           1       0.83      0.76      0.79      4548\n",
      "           2       0.56      0.27      0.36       838\n",
      "\n",
      "    accuracy                           0.80     11880\n",
      "   macro avg       0.73      0.64      0.67     11880\n",
      "weighted avg       0.79      0.80      0.79     11880\n",
      "\n",
      "Run time:  13:02:32.918131\n"
     ]
    }
   ],
   "source": [
    "#Round 1:\n",
    "hyper_xgb = dict(n_estimators=n_estimator_list, learning_rate=learning_rate_list, max_depth = max_depth_list)\n",
    "xgb_def = xgb.XGBClassifier(max_delta_step = 1, seed=42)\n",
    "model, df_results = create_model('XGBoost RandomSearch R1', xgb_def, dataset_type, X_train, X_test, y_train, y_test,\n",
    "                                               tuning_model = 'Random', hyperparams = hyper_xgb)\n",
    "\n",
    "df_results.to_csv(results_file, mode='a', header=False)\n",
    "df_all_results = pd.concat([df_all_results, df_results])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tuning greatly improved the results from the default model, and out-performed the best model except for the 'functional needs maintenance' class. The best number of estimators and max_depth were at the top of the ranges to try, however given the 13-hour runtime and the additional rounds of tuning required for more parameters however, further tuning of those two parameters will be left for a future exercise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this next round we are doing a random search over 24 parameter combinations. Considering the default model took 45 minutes, a lower-bound time estimate is 18 hours. However parameters from the first tuning round are included, making the model more complex and potentially more time-intensive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters:  [('gamma', 0), ('min_child_weight', 0), ('subsample', 0.8)]\n",
      "Confusion maxtrix: \n",
      " [[5812  553  129]\n",
      " [ 986 3501   61]\n",
      " [ 464  140  234]]\n",
      "Classification report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.89      0.85      6494\n",
      "           1       0.83      0.77      0.80      4548\n",
      "           2       0.55      0.28      0.37       838\n",
      "\n",
      "    accuracy                           0.80     11880\n",
      "   macro avg       0.73      0.65      0.67     11880\n",
      "weighted avg       0.80      0.80      0.79     11880\n",
      "\n",
      "Run time:  1 day, 14:20:53.984041\n"
     ]
    }
   ],
   "source": [
    "#Round 2:\n",
    "\n",
    "hyper_xgb = dict(min_child_weight = min_child_weight_list, gamma = gamma_list, subsample = subsample_list)\n",
    "\n",
    "#Use tuned parameters from round 1\n",
    "xgb_def = xgb.XGBClassifier(max_delta_step = 1, seed=42, \n",
    "                            n_estimators=800, learning_rate = 0.1, max_depth = 7) \n",
    "model, df_results = create_model('XGBoost RandomSearch R2', xgb_def, dataset_type, X_train, X_test, y_train, y_test,\n",
    "                                               tuning_model = 'Random', hyperparams = hyper_xgb)\n",
    "df_results.to_csv(results_file, mode='a', header=False)\n",
    "df_all_results = pd.concat([df_all_results, df_results])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results of the second round of hyperparameter tuning and cross validation produced the best results so far, but took 38 hours to run.  A third round of hyperparameter tuning was planned, but to make running the models more manageable, we will try another approach to feature reduction: Principle Component Analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Round 3: (postponing)\n",
    "hyper_xgb = dict(colsample_bytree = colsample_bytree_list, reg_alpha = reg_alpha_list)\n",
    "#Use tuned parameters from round 3\n",
    "xgb_def = xgb.XGBClassifier(max_delta_step = 1, seed=42, \n",
    "                            n_estimators=800, learning_rate=0.1,\n",
    "                            max_depth=7, min_child_weight=0,\n",
    "                            gamma=0, subsample=0.8)\n",
    "model, df_results = create_model('XGBoost RandomSearch R3', xgb_def, dataset_type, X_train, X_test, y_train, y_test,\n",
    "                                               tuning_model = 'Random', hyperparams = hyper_xgb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: PCA on tuned models and best dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Principal Component Analysis is a dimension-reduction technique that creates components representing expressions of variance in the data. Once the components are defined, a sub-set of components that contain most of the variance in the dataset can be used for modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.07202401 0.04785642 0.04542299 0.04147686 0.03300254 0.03268403\n",
      " 0.0304985  0.02802795 0.02622867 0.02589207 0.02425804 0.02384396\n",
      " 0.02318534 0.02139457 0.020804   0.02015062 0.02006747 0.0198935\n",
      " 0.01939686 0.01908511 0.01873668 0.01858928 0.01836523 0.01825386\n",
      " 0.01817128 0.01775561 0.01762026 0.01746519 0.01695737 0.01641924\n",
      " 0.01609718 0.01579318 0.01536496 0.01469567 0.01463354 0.01422444\n",
      " 0.01394791 0.01355722 0.01282698 0.01195956 0.01148766 0.01098055\n",
      " 0.01027743 0.00977989 0.00824909 0.0071453  0.00662466 0.00537655\n",
      " 0.00468223 0.00303795 0.00257738 0.00128189 0.00106349 0.00049743\n",
      " 0.0003104 ]\n"
     ]
    }
   ],
   "source": [
    "# create PCA instance\n",
    "pca = PCA()\n",
    "\n",
    "# fit on sample data\n",
    "pca.fit(X_train)\n",
    "\n",
    "# can use to transform sample data or test/unseen data\n",
    "Xpca_train_all = pca.transform(X_train)\n",
    "Xpca_test_all = pca.transform(X_test)\n",
    "\n",
    "# examine percent of variance explained by each principal component\n",
    "print(pca.explained_variance_/np.sum(pca.explained_variance_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(20,8), sharex = True)\n",
    "labels = ['pc' + str(i+1) for i in range(len(colnames))]\n",
    "ax = sns.barplot(labels, pca.explained_variance_, palette='muted')\n",
    "_ = plt.setp(ax.get_xticklabels(), rotation=45)\n",
    "_ = plt.title('Principal Components\\nfor Data')\n",
    "_ = plt.xlabel('Principal Component')\n",
    "_ = plt.ylabel('Explained Variance')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first leveling off point is after 13 components. We'll try re-running the best model with 13 components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=13)\n",
    "\n",
    "# fit on sample data\n",
    "pca.fit(X_train)\n",
    "\n",
    "# can use to transform sample data or test/unseen data\n",
    "Xpca_train = pca.transform(X_train)\n",
    "Xpca_test = pca.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will run the reduced data on the next-best performing classifier, the tuned RandomForest model, to determine if we see any improvement, before trying it on the long-running XGBoost model. Tuning the RandomForest model took 1 hour for GridSearch, 3 hours for RandomSearch, and 9 minutes with default parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion maxtrix: \n",
      " [[5478  784  232]\n",
      " [ 993 3448  107]\n",
      " [ 426  130  282]]\n",
      "Classification report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.84      0.82      6494\n",
      "           1       0.79      0.76      0.77      4548\n",
      "           2       0.45      0.34      0.39       838\n",
      "\n",
      "    accuracy                           0.78     11880\n",
      "   macro avg       0.68      0.65      0.66     11880\n",
      "weighted avg       0.77      0.78      0.77     11880\n",
      "\n",
      "Run time:  0:02:40.653694\n"
     ]
    }
   ],
   "source": [
    "# using best parameters determined from hyperparameter tuning and cross validation:\n",
    "# [('bootstrap', True), ('max_features', 'sqrt'), ('n_estimators', 800)]\n",
    "rfc = RandomForestClassifier(bootstrap=True, max_features='sqrt', n_estimators=800, random_state=42)\n",
    "rfc_model, df_results = create_model('Random Forest PCA', rfc, dataset_type, Xpca_train, Xpca_test, y_train, y_test, tuning_model='None')\n",
    "df_results.to_csv(results_file, mode='a', header=False)\n",
    "df_all_results = pd.concat([df_all_results, df_results])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model ran quickly but the results are not as good as the model produced before, where the F1 scores of class 0, 1 and 2 were 84, 80 and 39 respectively.  This could be because the cutoff of principal components was too aggressive.  We'll run the model again, this time using 38 components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion maxtrix: \n",
      " [[5461  803  230]\n",
      " [ 963 3472  113]\n",
      " [ 435  130  273]]\n",
      "Classification report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.84      0.82      6494\n",
      "           1       0.79      0.76      0.78      4548\n",
      "           2       0.44      0.33      0.38       838\n",
      "\n",
      "    accuracy                           0.77     11880\n",
      "   macro avg       0.68      0.64      0.66     11880\n",
      "weighted avg       0.77      0.77      0.77     11880\n",
      "\n",
      "Run time:  0:05:20.895886\n"
     ]
    }
   ],
   "source": [
    "pca = PCA(n_components=38)\n",
    "\n",
    "# fit on sample data\n",
    "pca.fit(X_train)\n",
    "\n",
    "# can use to transform sample data or test/unseen data\n",
    "Xpca_train_38 = pca.transform(X_train)\n",
    "Xpca_test_38 = pca.transform(X_test)\n",
    "rfc = RandomForestClassifier(bootstrap=True, max_features='sqrt', n_estimators=800, random_state=42)\n",
    "rfc_model, df_results = create_model('Random Forest PCA', rfc, dataset_type, Xpca_train_38, Xpca_test_38, y_train, y_test, tuning_model='None')\n",
    "df_results.to_csv(results_file, mode='a', header=False)\n",
    "df_all_results = pd.concat([df_all_results, df_results])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interestingly, increasing the number of components did not improve the results. In fact, they were worse. The F1 scores with more components were not as good as the scores with fewer components."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since performing dimension reduction with feature extraction via PCA did not improve the results on the next-best model, we will not attempt it on the top-performing model, XGBoost. Given the impractical duration of tuning that model further, we will leave that for future improvements."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
